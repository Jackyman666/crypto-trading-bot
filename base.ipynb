{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137cad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-6.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in /Users/jack/Desktop/DG/FaceRecognition/.conda/lib/python3.11/site-packages (from plotly) (23.2)\n",
      "Downloading plotly-6.3.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.10.1-py3-none-any.whl (419 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m419.5/419.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: narwhals, plotly\n",
      "Successfully installed narwhals-2.10.1 plotly-6.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "377d8d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing package in the notebook environment\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "840d77d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BTC = pd.read_csv('data_Jack_bianance/15min/BTCUSDT_15m.csv')\n",
    "df_DOGE = pd.read_csv('data_Jack_bianance/15min/DOGEUSDT_15m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "781dbaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Processing directory: /Users/jack/Desktop/crypto-trading-bot/data_Jack_bianance/5 min\n",
      "   Found 55 CSV files.\n",
      "â†’ Checking file: AAVE_5m.csv\n",
      "ðŸ”¤ Renamed: AAVE_5m.csv â†’ AAVEUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: AAVEUSDT_5m.csv\n",
      "âœ… Saved: AAVEUSDT_5m.csv | Dropped 55 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2020-10-15 03:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:45:00+00:00\n",
      "â†’ Checking file: ADA_5m.csv\n",
      "ðŸ”¤ Renamed: ADA_5m.csv â†’ ADAUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: ADAUSDT_5m.csv\n",
      "âœ… Saved: ADAUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:35:00+00:00\n",
      "â†’ Checking file: ALGO_5m.csv\n",
      "ðŸ”¤ Renamed: ALGO_5m.csv â†’ ALGOUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: ALGOUSDT_5m.csv\n",
      "âœ… Saved: ALGOUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:55:00+00:00\n",
      "â†’ Checking file: APT_5m.csv\n",
      "ðŸ”¤ Renamed: APT_5m.csv â†’ APTUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: APTUSDT_5m.csv\n",
      "âœ… Saved: APTUSDT_5m.csv | Dropped 94 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2022-10-19 01:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:50:00+00:00\n",
      "â†’ Checking file: ARB_5m.csv\n",
      "ðŸ”¤ Renamed: ARB_5m.csv â†’ ARBUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: ARBUSDT_5m.csv\n",
      "âœ… Saved: ARBUSDT_5m.csv | Dropped 101 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2023-03-23 15:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:50:00+00:00\n",
      "â†’ Checking file: ATOM_5m.csv\n",
      "ðŸ”¤ Renamed: ATOM_5m.csv â†’ ATOMUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: ATOMUSDT_5m.csv\n",
      "âœ… Saved: ATOMUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:00:00+00:00\n",
      "â†’ Checking file: AVAX_5m.csv\n",
      "ðŸ”¤ Renamed: AVAX_5m.csv â†’ AVAXUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: AVAXUSDT_5m.csv\n",
      "âœ… Saved: AVAXUSDT_5m.csv | Dropped 49 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2020-09-22 06:30:00+00:00\n",
      "   last  ts: 2025-11-05 15:45:00+00:00\n",
      "â†’ Checking file: BCH_5m.csv\n",
      "ðŸ”¤ Renamed: BCH_5m.csv â†’ BCHUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: BCHUSDT_5m.csv\n",
      "âœ… Saved: BCHUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:40:00+00:00\n",
      "â†’ Checking file: BNB_5m.csv\n",
      "ðŸ”¤ Renamed: BNB_5m.csv â†’ BNBUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: BNBUSDT_5m.csv\n",
      "âœ… Saved: BNBUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:35:00+00:00\n",
      "â†’ Checking file: BNSOL_5m.csv\n",
      "ðŸ”¤ Renamed: BNSOL_5m.csv â†’ BNSOLUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: BNSOLUSDT_5m.csv\n",
      "âœ… Saved: BNSOLUSDT_5m.csv | Dropped 95 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2024-10-17 08:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:05:00+00:00\n",
      "â†’ Checking file: BONK_5m.csv\n",
      "ðŸ”¤ Renamed: BONK_5m.csv â†’ BONKUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: BONKUSDT_5m.csv\n",
      "âœ… Saved: BONKUSDT_5m.csv | Dropped 199192 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "â†’ Checking file: BTC_5m.csv\n",
      "ðŸ”¤ Renamed: BTC_5m.csv â†’ BTCUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: BTCUSDT_5m.csv\n",
      "âœ… Saved: BTCUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:30:00+00:00\n",
      "â†’ Checking file: DOGE_5m.csv\n",
      "ðŸ”¤ Renamed: DOGE_5m.csv â†’ DOGEUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: DOGEUSDT_5m.csv\n",
      "âœ… Saved: DOGEUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:35:00+00:00\n",
      "â†’ Checking file: DOT_5m.csv\n",
      "ðŸ”¤ Renamed: DOT_5m.csv â†’ DOTUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: DOTUSDT_5m.csv\n",
      "âœ… Saved: DOTUSDT_5m.csv | Dropped 46 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2020-08-18 23:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:45:00+00:00\n",
      "â†’ Checking file: ENA_5m.csv\n",
      "ðŸ”¤ Renamed: ENA_5m.csv â†’ ENAUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: ENAUSDT_5m.csv\n",
      "âœ… Saved: ENAUSDT_5m.csv | Dropped 96 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2024-04-02 08:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:00:00+00:00\n",
      "â†’ Checking file: ENS_5m.csv\n",
      "ðŸ”¤ Renamed: ENS_5m.csv â†’ ENSUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: ENSUSDT_5m.csv\n",
      "âœ… Saved: ENSUSDT_5m.csv | Dropped 90 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2021-11-10 07:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:00:00+00:00\n",
      "â†’ Checking file: ETC_5m.csv\n",
      "ðŸ”¤ Renamed: ETC_5m.csv â†’ ETCUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: ETCUSDT_5m.csv\n",
      "âœ… Saved: ETCUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:55:00+00:00\n",
      "â†’ Checking file: ETH_5m.csv\n",
      "ðŸ”¤ Renamed: ETH_5m.csv â†’ ETHUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: ETHUSDT_5m.csv\n",
      "âœ… Saved: ETHUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:30:00+00:00\n",
      "â†’ Checking file: FET_5m.csv\n",
      "ðŸ”¤ Renamed: FET_5m.csv â†’ FETUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: FETUSDT_5m.csv\n",
      "âœ… Saved: FETUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:00:00+00:00\n",
      "â†’ Checking file: FIL_5m.csv\n",
      "ðŸ”¤ Renamed: FIL_5m.csv â†’ FILUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: FILUSDT_5m.csv\n",
      "âœ… Saved: FILUSDT_5m.csv | Dropped 531690 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "â†’ Checking file: FORM_5m.csv\n",
      "ðŸ”¤ Renamed: FORM_5m.csv â†’ FORMUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: FORMUSDT_5m.csv\n",
      "âœ… Saved: FORMUSDT_5m.csv | Dropped 102 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2025-03-19 08:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:10:00+00:00\n",
      "â†’ Checking file: HBAR_5m.csv\n",
      "ðŸ”¤ Renamed: HBAR_5m.csv â†’ HBARUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: HBARUSDT_5m.csv\n",
      "âœ… Saved: HBARUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:40:00+00:00\n",
      "â†’ Checking file: ICP_5m.csv\n",
      "ðŸ”¤ Renamed: ICP_5m.csv â†’ ICPUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: ICPUSDT_5m.csv\n",
      "âœ… Saved: ICPUSDT_5m.csv | Dropped 79 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2021-05-11 01:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:50:00+00:00\n",
      "â†’ Checking file: INJ_5m.csv\n",
      "ðŸ”¤ Renamed: INJ_5m.csv â†’ INJUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: INJUSDT_5m.csv\n",
      "âœ… Saved: INJUSDT_5m.csv | Dropped 52 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2020-10-21 04:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:10:00+00:00\n",
      "â†’ Checking file: JTO_5m.csv\n",
      "ðŸ”¤ Renamed: JTO_5m.csv â†’ JTOUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: JTOUSDT_5m.csv\n",
      "âœ… Saved: JTOUSDT_5m.csv | Dropped 92 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2023-12-07 16:30:00+00:00\n",
      "   last  ts: 2025-11-05 16:05:00+00:00\n",
      "â†’ Checking file: JUP_5m.csv\n",
      "ðŸ”¤ Renamed: JUP_5m.csv â†’ JUPUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: JUPUSDT_5m.csv\n",
      "âœ… Saved: JUPUSDT_5m.csv | Dropped 185560 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "â†’ Checking file: LINK_5m.csv\n",
      "ðŸ”¤ Renamed: LINK_5m.csv â†’ LINKUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: LINKUSDT_5m.csv\n",
      "âœ… Saved: LINKUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:40:00+00:00\n",
      "â†’ Checking file: LTC_5m.csv\n",
      "ðŸ”¤ Renamed: LTC_5m.csv â†’ LTCUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: LTCUSDT_5m.csv\n",
      "âœ… Saved: LTCUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:45:00+00:00\n",
      "â†’ Checking file: NEAR_5m.csv\n",
      "ðŸ”¤ Renamed: NEAR_5m.csv â†’ NEARUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: NEARUSDT_5m.csv\n",
      "âœ… Saved: NEARUSDT_5m.csv | Dropped 50 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2020-10-14 05:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:50:00+00:00\n",
      "â†’ Checking file: ONDO_5m.csv\n",
      "ðŸ”¤ Renamed: ONDO_5m.csv â†’ ONDOUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: ONDOUSDT_5m.csv\n",
      "âœ… Saved: ONDOUSDT_5m.csv | Dropped 108 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2025-04-11 14:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:50:00+00:00\n",
      "â†’ Checking file: OP_5m.csv\n",
      "ðŸ”¤ Renamed: OP_5m.csv â†’ OPUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: OPUSDT_5m.csv\n",
      "âœ… Saved: OPUSDT_5m.csv | Dropped 95 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2022-06-01 08:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:55:00+00:00\n",
      "â†’ Checking file: PEPE_5m.csv\n",
      "ðŸ”¤ Renamed: PEPE_5m.csv â†’ PEPEUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: PEPEUSDT_5m.csv\n",
      "âœ… Saved: PEPEUSDT_5m.csv | Dropped 90 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2023-05-05 18:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:45:00+00:00\n",
      "â†’ Checking file: POL_5m.csv\n",
      "ðŸ”¤ Renamed: POL_5m.csv â†’ POLUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: POLUSDT_5m.csv\n",
      "âœ… Saved: POLUSDT_5m.csv | Dropped 91 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2024-09-13 10:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:55:00+00:00\n",
      "â†’ Checking file: QNT_5m.csv\n",
      "ðŸ”¤ Renamed: QNT_5m.csv â†’ QNTUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: QNTUSDT_5m.csv\n",
      "âœ… Saved: QNTUSDT_5m.csv | Dropped 92 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2021-07-29 06:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:10:00+00:00\n",
      "â†’ Checking file: RAY_5m.csv\n",
      "ðŸ”¤ Renamed: RAY_5m.csv â†’ RAYUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: RAYUSDT_5m.csv\n",
      "âœ… Saved: RAYUSDT_5m.csv | Dropped 83 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2021-08-10 06:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:05:00+00:00\n",
      "â†’ Checking file: RENDER_5m.csv\n",
      "ðŸ”¤ Renamed: RENDER_5m.csv â†’ RENDERUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: RENDERUSDT_5m.csv\n",
      "âœ… Saved: RENDERUSDT_5m.csv | Dropped 134686 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "â†’ Checking file: SEI_5m.csv\n",
      "ðŸ”¤ Renamed: SEI_5m.csv â†’ SEIUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: SEIUSDT_5m.csv\n",
      "âœ… Saved: SEIUSDT_5m.csv | Dropped 234290 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "â†’ Checking file: SHIB_5m.csv\n",
      "ðŸ”¤ Renamed: SHIB_5m.csv â†’ SHIBUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: SHIBUSDT_5m.csv\n",
      "âœ… Saved: SHIBUSDT_5m.csv | Dropped 82 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2021-05-10 11:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:45:00+00:00\n",
      "â†’ Checking file: SKY_5m.csv\n",
      "ðŸ”¤ Renamed: SKY_5m.csv â†’ SKYUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: SKYUSDT_5m.csv\n",
      "âœ… Saved: SKYUSDT_5m.csv | Dropped 14312 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "â†’ Checking file: SOL_5m.csv\n",
      "ðŸ”¤ Renamed: SOL_5m.csv â†’ SOLUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: SOLUSDT_5m.csv\n",
      "âœ… Saved: SOLUSDT_5m.csv | Dropped 41 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2020-08-11 06:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:35:00+00:00\n",
      "â†’ Checking file: STX_5m.csv\n",
      "ðŸ”¤ Renamed: STX_5m.csv â†’ STXUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: STXUSDT_5m.csv\n",
      "âœ… Saved: STXUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:10:00+00:00\n",
      "â†’ Checking file: SUI_5m.csv\n",
      "ðŸ”¤ Renamed: SUI_5m.csv â†’ SUIUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: SUIUSDT_5m.csv\n",
      "âœ… Saved: SUIUSDT_5m.csv | Dropped 93 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2023-05-03 12:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:40:00+00:00\n",
      "â†’ Checking file: TAO_5m.csv\n",
      "ðŸ”¤ Renamed: TAO_5m.csv â†’ TAOUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: TAOUSDT_5m.csv\n",
      "âœ… Saved: TAOUSDT_5m.csv | Dropped 165169 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "â†’ Checking file: TIA_5m.csv\n",
      "ðŸ”¤ Renamed: TIA_5m.csv â†’ TIAUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: TIAUSDT_5m.csv\n",
      "âœ… Saved: TIAUSDT_5m.csv | Dropped 95 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2023-10-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:05:00+00:00\n",
      "â†’ Checking file: TON_5m.csv\n",
      "ðŸ”¤ Renamed: TON_5m.csv â†’ TONUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: TONUSDT_5m.csv\n",
      "âœ… Saved: TONUSDT_5m.csv | Dropped 94 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2024-08-08 10:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:45:00+00:00\n",
      "â†’ Checking file: TRUMP_5m.csv\n",
      "ðŸ”¤ Renamed: TRUMP_5m.csv â†’ TRUMPUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: TRUMPUSDT_5m.csv\n",
      "âœ… Saved: TRUMPUSDT_5m.csv | Dropped 102 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2025-01-19 08:30:00+00:00\n",
      "   last  ts: 2025-11-05 15:55:00+00:00\n",
      "â†’ Checking file: TRX_5m.csv\n",
      "ðŸ”¤ Renamed: TRX_5m.csv â†’ TRXUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: TRXUSDT_5m.csv\n",
      "âœ… Saved: TRXUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:35:00+00:00\n",
      "â†’ Checking file: UNI_5m.csv\n",
      "ðŸ”¤ Renamed: UNI_5m.csv â†’ UNIUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: UNIUSDT_5m.csv\n",
      "âœ… Saved: UNIUSDT_5m.csv | Dropped 47 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2020-09-17 03:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:40:00+00:00\n",
      "â†’ Checking file: VET_5m.csv\n",
      "ðŸ”¤ Renamed: VET_5m.csv â†’ VETUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: VETUSDT_5m.csv\n",
      "âœ… Saved: VETUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:00:00+00:00\n",
      "â†’ Checking file: VIRTUAL_5m.csv\n",
      "ðŸ”¤ Renamed: VIRTUAL_5m.csv â†’ VIRTUALUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: VIRTUALUSDT_5m.csv\n",
      "âœ… Saved: VIRTUALUSDT_5m.csv | Dropped 87 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2025-04-11 14:00:00+00:00\n",
      "   last  ts: 2025-11-05 16:05:00+00:00\n",
      "â†’ Checking file: WBETH_5m.csv\n",
      "ðŸ”¤ Renamed: WBETH_5m.csv â†’ WBETHUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: WBETHUSDT_5m.csv\n",
      "âœ… Saved: WBETHUSDT_5m.csv | Dropped 109 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2023-07-19 06:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:40:00+00:00\n",
      "â†’ Checking file: WBTC_5m.csv\n",
      "ðŸ”¤ Renamed: WBTC_5m.csv â†’ WBTCUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: WBTCUSDT_5m.csv\n",
      "âœ… Saved: WBTCUSDT_5m.csv | Dropped 265698 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "â†’ Checking file: WLD_5m.csv\n",
      "ðŸ”¤ Renamed: WLD_5m.csv â†’ WLDUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: WLDUSDT_5m.csv\n",
      "âœ… Saved: WLDUSDT_5m.csv | Dropped 95 bad rows | dtype float64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2023-07-24 09:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:50:00+00:00\n",
      "â†’ Checking file: XLM_5m.csv\n",
      "ðŸ”¤ Renamed: XLM_5m.csv â†’ XLMUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: XLMUSDT_5m.csv\n",
      "âœ… Saved: XLMUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:35:00+00:00\n",
      "â†’ Checking file: XRP_5m.csv\n",
      "ðŸ”¤ Renamed: XRP_5m.csv â†’ XRPUSDT_5m.csv\n",
      "\n",
      "ðŸ“„ Cleaning file: XRPUSDT_5m.csv\n",
      "âœ… Saved: XRPUSDT_5m.csv | Dropped 0 bad rows | dtype int64 â†’ datetime64[ns, UTC]\n",
      "   first ts: 2019-12-31 16:00:00+00:00\n",
      "   last  ts: 2025-11-05 15:30:00+00:00\n",
      "\n",
      "ðŸŽ¯ Batch operation complete.\n"
     ]
    }
   ],
   "source": [
    "# Batch rename (add USDT) + timestamp clean/convert (delete corrupted rows)\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------\n",
    "# Settings\n",
    "# ----------------------------------\n",
    "CANDIDATE_DIRS = [\n",
    "    Path(\"data_Jack_bianance/5 min\"),\n",
    "    Path(\"data_Jack_bianance/5min\"),\n",
    "]\n",
    "\n",
    "SYMBOL_RE = r\"[A-Za-z0-9\\-]+\"   # letters/numbers/hyphen in symbol names\n",
    "TIMESTAMP_COL = \"timestamp\"\n",
    "START_UTC = pd.Timestamp(\"1970-01-01\", tz=\"UTC\")\n",
    "END_UTC   = pd.Timestamp(\"2100-01-01\", tz=\"UTC\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Filename helpers\n",
    "# ----------------------------------\n",
    "def is_csv_5m(name: str) -> bool:\n",
    "    \"\"\"Match SYMBOL_5m.csv or SYMBOLUSDT_5m.csv\"\"\"\n",
    "    return bool(re.match(fr\"^{SYMBOL_RE}(?:USDT)?_5m\\.csv$\", name))\n",
    "\n",
    "def target_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    DOGE_5m.csv      -> DOGEUSDT_5m.csv\n",
    "    DOGEUSDT_5m.csv  -> (unchanged)\n",
    "    \"\"\"\n",
    "    m = re.match(fr\"^({SYMBOL_RE})_5m\\.csv$\", name)\n",
    "    if m:\n",
    "        sym = m.group(1)\n",
    "        return f\"{sym}USDT_5m.csv\"\n",
    "    m2 = re.match(fr\"^({SYMBOL_RE})USDT_5m\\.csv$\", name)\n",
    "    if m2:\n",
    "        return name\n",
    "    return name  # non-matching: leave as-is\n",
    "\n",
    "def rename_to_usdt(csv_path: Path) -> Path:\n",
    "    new_name = target_name(csv_path.name)\n",
    "    if new_name != csv_path.name:\n",
    "        new_path = csv_path.with_name(new_name)\n",
    "        try:\n",
    "            csv_path.rename(new_path)\n",
    "            print(f\"ðŸ”¤ Renamed: {csv_path.name} â†’ {new_name}\")\n",
    "            return new_path\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Rename failed for {csv_path.name}: {e}\")\n",
    "    else:\n",
    "        print(f\"   â†³ No rename needed: {csv_path.name}\")\n",
    "    return csv_path\n",
    "\n",
    "# ----------------------------------\n",
    "# Timestamp cleaning\n",
    "# ----------------------------------\n",
    "def convert_and_clean_timestamp(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert 'timestamp' to timezone-aware UTC datetimes.\n",
    "    - Accepts numeric epoch values (s/ms/us/ns) or ISO-like strings.\n",
    "    - Deletes (returns NaT for) corrupted/out-of-range values.\n",
    "    \"\"\"\n",
    "    # Try numeric conversion first (keeps NaN where not numeric)\n",
    "    s_num = pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "    out = pd.Series(pd.NaT, index=series.index, dtype=\"datetime64[ns, UTC]\")\n",
    "\n",
    "    # 1) Handle ISO-like (non-numeric) entries\n",
    "    non_numeric_mask = s_num.isna() & series.notna()\n",
    "    if non_numeric_mask.any():\n",
    "        dt_iso = pd.to_datetime(series[non_numeric_mask], utc=True, errors=\"coerce\")\n",
    "        out.loc[non_numeric_mask] = dt_iso\n",
    "\n",
    "    # 2) Numeric-like entries: choose unit by magnitude per-row\n",
    "    num_mask = s_num.notna()\n",
    "    if num_mask.any():\n",
    "        x = s_num[num_mask]\n",
    "\n",
    "        # magnitude buckets (typical API ranges)\n",
    "        m_ms = (x >= 1e12) & (x < 1e13)  # milliseconds\n",
    "        m_s  = (x >= 1e9)  & (x < 1e10)  # seconds\n",
    "        m_us = (x >= 1e15) & (x < 1e16)  # microseconds\n",
    "        m_ns = (x >= 1e18) & (x < 1e19)  # nanoseconds\n",
    "\n",
    "        if m_ms.any():\n",
    "            out.loc[m_ms.index] = pd.to_datetime(x[m_ms], unit=\"ms\", utc=True, errors=\"coerce\")\n",
    "        if m_s.any():\n",
    "            out.loc[m_s.index]  = pd.to_datetime(x[m_s],  unit=\"s\",  utc=True, errors=\"coerce\")\n",
    "        if m_us.any():\n",
    "            out.loc[m_us.index] = pd.to_datetime(x[m_us], unit=\"us\", utc=True, errors=\"coerce\")\n",
    "        if m_ns.any():\n",
    "            out.loc[m_ns.index] = pd.to_datetime(x[m_ns], unit=\"ns\", utc=True, errors=\"coerce\")\n",
    "\n",
    "    # 3) Bounds check (delete absurd dates)\n",
    "    bad_bounds = out.notna() & ~out.between(START_UTC, END_UTC)\n",
    "    if bad_bounds.any():\n",
    "        out[bad_bounds] = pd.NaT\n",
    "\n",
    "    return out\n",
    "\n",
    "def clean_csv_inplace(csv_path: Path):\n",
    "    \"\"\"Load CSV, convert TIMESTAMP_COL, drop corrupted rows, and save back.\"\"\"\n",
    "    print(f\"\\nðŸ“„ Cleaning file: {csv_path.name}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Read error, skipping {csv_path.name}: {e}\")\n",
    "        return\n",
    "\n",
    "    if TIMESTAMP_COL not in df.columns:\n",
    "        print(f\"âš ï¸  No '{TIMESTAMP_COL}' column. Skipped {csv_path.name}.\")\n",
    "        return\n",
    "\n",
    "    before_rows = len(df)\n",
    "    before_dtype = df[TIMESTAMP_COL].dtype\n",
    "\n",
    "    dt = convert_and_clean_timestamp(df[TIMESTAMP_COL])\n",
    "    df[TIMESTAMP_COL] = dt\n",
    "\n",
    "    # Drop rows with NaT in timestamp\n",
    "    df = df[df[TIMESTAMP_COL].notna()].copy()\n",
    "    dropped = before_rows - len(df)\n",
    "\n",
    "    # Put timestamp first (optional, tidy)\n",
    "    cols = list(df.columns)\n",
    "    if cols[0] != TIMESTAMP_COL:\n",
    "        cols.remove(TIMESTAMP_COL)\n",
    "        df = df[[TIMESTAMP_COL] + cols]\n",
    "\n",
    "    try:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"âœ… Saved: {csv_path.name} | Dropped {dropped} bad rows \"\n",
    "              f\"| dtype {before_dtype} â†’ {df[TIMESTAMP_COL].dtype}\")\n",
    "        if len(df):\n",
    "            print(f\"   first ts: {df[TIMESTAMP_COL].iloc[0]}\")\n",
    "            print(f\"   last  ts: {df[TIMESTAMP_COL].iloc[-1]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Write error for {csv_path.name}: {e}\")\n",
    "\n",
    "# ----------------------------------\n",
    "# Main batch run\n",
    "# ----------------------------------\n",
    "found_any = False\n",
    "for d in CANDIDATE_DIRS:\n",
    "    if not d.exists():\n",
    "        continue\n",
    "    found_any = True\n",
    "    print(f\"\\nðŸ“ Processing directory: {d.resolve()}\")\n",
    "    files = sorted(d.glob(\"*.csv\"))\n",
    "    print(f\"   Found {len(files)} CSV files.\")\n",
    "\n",
    "    for csv_path in files:\n",
    "        print(f\"â†’ Checking file: {csv_path.name}\")\n",
    "        if not is_csv_5m(csv_path.name):\n",
    "            print(\"   âš ï¸ Skipped (does not match *_5m.csv pattern)\")\n",
    "            continue\n",
    "\n",
    "        # 1) Rename to add USDT if needed\n",
    "        csv_path = rename_to_usdt(csv_path)\n",
    "\n",
    "        # 2) Convert timestamp + delete corrupted rows\n",
    "        clean_csv_inplace(csv_path)\n",
    "\n",
    "if not found_any:\n",
    "    print(\"âŒ No matching directory found. Checked:\\n - data_Jack_bianance/5 min\\n - data_Jack_bianance/5min\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Batch operation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007aeaac",
   "metadata": {},
   "source": [
    "# Trend Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd6ff6f",
   "metadata": {},
   "source": [
    "### Calculate SMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "56b6020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_SMA20(df, column_name='close', period = 20):\n",
    "\n",
    "    return df[column_name].rolling(window=period).mean()\n",
    "\n",
    "def calculate_SMA50(df, column_name='close', period = 50):\n",
    "    \n",
    "    return df[column_name].rolling(window=period).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b488fbc4",
   "metadata": {},
   "source": [
    "### Calculate EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cc24fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_EMA20(df, column_name: str = 'close', period: int = 20):\n",
    "\n",
    "    return df[column_name].ewm(span=period, adjust=False).mean()\n",
    "\n",
    "def calculate_EMA50(df, column_name: str = 'close', period: int = 50):\n",
    "\n",
    "    return df[column_name].ewm(span=period, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "016058b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        timestamp       open       high        low      close  \\\n",
      "0       2020-01-01 00:00:00+00:00    7195.24    7196.25    7178.20    7180.97   \n",
      "1       2020-01-01 00:15:00+00:00    7180.97    7186.40    7175.47    7178.45   \n",
      "2       2020-01-01 00:30:00+00:00    7178.19    7185.44    7176.23    7179.56   \n",
      "3       2020-01-01 00:45:00+00:00    7179.35    7183.98    7175.46    7177.02   \n",
      "4       2020-01-01 01:00:00+00:00    7176.47    7194.04    7175.71    7190.86   \n",
      "...                           ...        ...        ...        ...        ...   \n",
      "205054  2025-11-07 13:45:00+00:00  100203.60  100579.72  100182.97  100347.35   \n",
      "205055  2025-11-07 14:00:00+00:00  100347.35  100531.78  100018.61  100352.32   \n",
      "205056  2025-11-07 14:15:00+00:00  100352.32  100370.82  100058.00  100296.70   \n",
      "205057  2025-11-07 14:30:00+00:00  100296.70  100793.33   99519.32  100793.33   \n",
      "205058  2025-11-07 14:45:00+00:00  100793.65  100922.78  100316.24  100790.29   \n",
      "\n",
      "            volume       SMA_20       SMA_50         EMA_20         EMA_50  \n",
      "0       202.942868          NaN          NaN    7180.970000    7180.970000  \n",
      "1       128.242654          NaN          NaN    7180.730000    7180.871176  \n",
      "2        83.487458          NaN          NaN    7180.618571    7180.819758  \n",
      "3        97.141921          NaN          NaN    7180.275850    7180.670748  \n",
      "4       103.520522          NaN          NaN    7181.283865    7181.070326  \n",
      "...            ...          ...          ...            ...            ...  \n",
      "205054  285.204290  100433.0395  101322.3840  100471.322040  101021.544092  \n",
      "205055  441.883870  100382.1060  101307.7090  100459.988513  100995.300010  \n",
      "205056  353.774910  100346.6280  101282.8638  100444.437226  100967.903931  \n",
      "205057  827.187760  100339.2710  101267.1814  100477.665109  100961.057894  \n",
      "205058  946.197180  100328.1025  101258.1270  100507.438908  100954.361114  \n",
      "\n",
      "[205059 rows x 10 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "df_BTC['SMA_20'] = calculate_SMA20(df_BTC)\n",
    "df_BTC['SMA_50'] = calculate_SMA50(df_BTC)\n",
    "\n",
    "df_BTC['EMA_20'] = calculate_EMA20(df_BTC)\n",
    "df_BTC['EMA_50'] = calculate_EMA50(df_BTC)\n",
    "\n",
    "print(df_BTC)\n",
    "print(type(df_BTC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2c539ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Trend Detection Column ---\n",
    "def detect_trend_series(df, sma_short='SMA_20', sma_long='SMA_50', window=5):\n",
    "    trend_list = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if i < window:\n",
    "            trend_list.append(np.nan)\n",
    "        else:\n",
    "            recent = df.iloc[i-window+1:i+1]\n",
    "            if all(recent[sma_short] > recent[sma_long]):\n",
    "                trend_list.append(\"bullish\")\n",
    "            elif all(recent[sma_short] < recent[sma_long]):\n",
    "                trend_list.append(\"bearish\")\n",
    "            else:\n",
    "                trend_list.append(\"volatile\")\n",
    "    \n",
    "    return trend_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "024e5cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            close       SMA_20       SMA_50     trend\n",
      "205029  101826.94  102079.1800  101583.5458   bullish\n",
      "205030  102010.00  102084.4245  101587.4784   bullish\n",
      "205031  101934.67  102064.6000  101596.9438   bullish\n",
      "205032  101813.04  102040.9665  101605.4284   bullish\n",
      "205033  101621.45  102015.5375  101611.7074   bullish\n",
      "205034  101496.18  101994.5320  101619.9862   bullish\n",
      "205035  101370.99  101983.9350  101632.1568   bullish\n",
      "205036  101006.26  101934.4700  101633.9998   bullish\n",
      "205037  100940.47  101890.0195  101640.7058   bullish\n",
      "205038  101013.66  101839.3430  101643.5128   bullish\n",
      "205039  100958.44  101765.5935  101642.4874   bullish\n",
      "205040  100668.32  101678.6100  101633.5104   bullish\n",
      "205041  100944.85  101610.5910  101632.2134  volatile\n",
      "205042  100770.85  101526.7600  101624.9684  volatile\n",
      "205043  100103.38  101415.7935  101602.9282  volatile\n",
      "205044   99918.02  101299.1945  101572.9876  volatile\n",
      "205045  100155.02  101202.9040  101547.7932   bearish\n",
      "205046  100411.89  101132.5435  101528.8510   bearish\n",
      "205047  100293.50  101055.0130  101509.8150   bearish\n",
      "205048   99932.01  100959.4970  101481.5344   bearish\n",
      "205049   99799.98  100858.1490  101450.5828   bearish\n",
      "205050   99638.28  100739.5630  101424.7562   bearish\n",
      "205051  100043.59  100645.0090  101398.0624   bearish\n",
      "205052  100140.33  100561.3735  101371.2734   bearish\n",
      "205053  100203.60  100490.4810  101349.0418   bearish\n",
      "205054  100347.35  100433.0395  101322.3840   bearish\n",
      "205055  100352.32  100382.1060  101307.7090   bearish\n",
      "205056  100296.70  100346.6280  101282.8638   bearish\n",
      "205057  100793.33  100339.2710  101267.1814   bearish\n",
      "205058  100790.29  100328.1025  101258.1270   bearish\n"
     ]
    }
   ],
   "source": [
    "df_BTC['trend'] = detect_trend_series(df_BTC)\n",
    "\n",
    "print(df_BTC[['close', 'SMA_20', 'SMA_50', 'trend']].tail(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2e0800",
   "metadata": {},
   "source": [
    "# Find Pivot Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b0c0938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pivots(df: pd.DataFrame, window: int = 2, min_gap: int = 1) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Detect pivot (fractal) points in OHLC data, then thin so that in any\n",
    "    `min_gap` consecutive bars there is at most one pivot (of any type).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: 0 = no pivot, 1 = pivot high, 2 = pivot low, 3 = both.\n",
    "    \"\"\"\n",
    "    pivots = [0] * len(df)\n",
    "\n",
    "    for candle in range(len(df)):\n",
    "        # skip edges\n",
    "        if candle - window < 0 or candle + window >= len(df):\n",
    "            continue\n",
    "\n",
    "        pivotHigh = True\n",
    "        pivotLow = True\n",
    "\n",
    "        # check neighborhood [candle-window, candle+window]\n",
    "        for i in range(candle - window, candle + window + 1):\n",
    "            if df.iloc[candle].low > df.iloc[i].low:\n",
    "                pivotLow = False\n",
    "            if df.iloc[candle].high < df.iloc[i].high:\n",
    "                pivotHigh = False\n",
    "\n",
    "        if pivotHigh and pivotLow:\n",
    "            pivots[candle] = 3 # both pivot high and low (rare case) \n",
    "        elif pivotHigh:\n",
    "            pivots[candle] = 1  # pivot high\n",
    "        elif pivotLow:\n",
    "            pivots[candle] = 2  # pivot low\n",
    "\n",
    "    # === Spacing enforcement: at most one pivot in any `min_gap` bars ===\n",
    "    # Treat any non-zero (1,2,3) as a pivot; keep the first, drop others within the next (min_gap-1) bars.\n",
    "    piv = np.array(pivots, dtype=int)\n",
    "    last_kept = -10**9  # sufficiently negative\n",
    "\n",
    "    # indices of any pivot (1,2,3), in chronological order\n",
    "    pivot_idx = np.where(piv != 0)[0]\n",
    "\n",
    "    for idx in pivot_idx:\n",
    "        if idx - last_kept >= min_gap:\n",
    "            last_kept = idx      # keep this pivot\n",
    "        else:\n",
    "            piv[idx] = 0         # too close to previous pivot â†’ drop\n",
    "\n",
    "    return pd.Series(piv, index=df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a7c40c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DOGE['isPivot'] = find_pivots(df_DOGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27416509",
   "metadata": {},
   "source": [
    "# Finding Support + Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "def5d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- utilities ----------\n",
    "def prep_df(df: pd.DataFrame, ts_col: str = 'timestamp') -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if ts_col in out.columns:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(out[ts_col]):\n",
    "            out[ts_col] = pd.to_datetime(out[ts_col], errors='coerce')\n",
    "        if pd.api.types.is_datetime64tz_dtype(out[ts_col].dtype):\n",
    "            out[ts_col] = out[ts_col].dt.tz_convert('UTC').dt.tz_localize(None)\n",
    "    return out\n",
    "\n",
    "def compute_pivots_if_missing(df: pd.DataFrame, window: int, min_gap: int, pivot_col='isPivot') -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if pivot_col not in out.columns:\n",
    "        out[pivot_col] = find_pivots(out, window=window, min_gap=min_gap)  # uses your function\n",
    "    return out\n",
    "\n",
    "def within_pct(a: float, b: float, pct: float) -> bool:\n",
    "    if b == 0 or np.isnan(a) or np.isnan(b): return False\n",
    "    return abs(a - b) / b <= pct\n",
    "\n",
    "# ---------- Step 1: find support/resistance levels from pairs of close pivots ----------\n",
    "def find_close_pivot_levels(df: pd.DataFrame,\n",
    "                            tol_pct: float = 0.0025,   # 0.5%\n",
    "                            pivot_col: str = 'isPivot'):\n",
    "    \"\"\"\n",
    "    Find levels by pairing consecutive pivots of the SAME type that are within tol_pct in price.\n",
    "    - Support: two pivot LOWS within tol_pct -> level = average(low1, low2)\n",
    "    - Resistance: two pivot HIGHS within tol_pct -> level = average(high1, high2)\n",
    "    Returns: lists of dicts for supports and resistances.\n",
    "    \"\"\"\n",
    "    supports, resistances = [], []\n",
    "\n",
    "    # collect indices by type (strict; treat code 3 as both)\n",
    "    low_idxs = df.index[df[pivot_col].isin([2, 3])].to_list()\n",
    "    high_idxs = df.index[df[pivot_col].isin([1, 3])].to_list()\n",
    "\n",
    "    # pair consecutive lows\n",
    "    for i in range(len(low_idxs) - 1):\n",
    "        i1, i2 = low_idxs[i], low_idxs[i+1]\n",
    "        p1, p2 = float(df.at[i1, 'low']), float(df.at[i2, 'low'])\n",
    "        if within_pct(p2, p1, tol_pct):\n",
    "            lvl = (p1 + p2) / 2.0\n",
    "            supports.append({\n",
    "                'level': lvl,\n",
    "                'first_idx': df.index.get_loc(i1),\n",
    "                'second_idx': df.index.get_loc(i2),\n",
    "                'first_time': df.at[i1, 'timestamp'] if 'timestamp' in df.columns else i1,\n",
    "                'second_time': df.at[i2, 'timestamp'] if 'timestamp' in df.columns else i2,\n",
    "                'first_price': p1,\n",
    "                'second_price': p2\n",
    "            })\n",
    "\n",
    "    # pair consecutive highs\n",
    "    for i in range(len(high_idxs) - 1):\n",
    "        i1, i2 = high_idxs[i], high_idxs[i+1]\n",
    "        p1, p2 = float(df.at[i1, 'high']), float(df.at[i2, 'high'])\n",
    "        if within_pct(p2, p1, tol_pct):\n",
    "            lvl = (p1 + p2) / 2.0\n",
    "            resistances.append({\n",
    "                'level': lvl,\n",
    "                'first_idx': df.index.get_loc(i1),\n",
    "                'second_idx': df.index.get_loc(i2),\n",
    "                'first_time': df.at[i1, 'timestamp'] if 'timestamp' in df.columns else i1,\n",
    "                'second_time': df.at[i2, 'timestamp'] if 'timestamp' in df.columns else i2,\n",
    "                'first_price': p1,\n",
    "                'second_price': p2\n",
    "            })\n",
    "\n",
    "    return supports, resistances\n",
    "\n",
    "# ---------- helper: previous pivot high before a given absolute row index ----------\n",
    "def prev_pivot_high_before(df: pd.DataFrame, abs_row: int, pivot_col: str = 'isPivot'):\n",
    "    # abs_row is positional index (0..len-1). Convert to actual df index label:\n",
    "    for pos in range(abs_row - 1, -1, -1):\n",
    "        label = df.index[pos]\n",
    "        piv = int(df.at[label, pivot_col])\n",
    "        if piv in (1, 3):\n",
    "            return pos, float(df.at[label, 'high'])\n",
    "    return None, None\n",
    "\n",
    "# ---------- Step 2 (for supports): scan window for PL1 then PH2 confirmation ----------\n",
    "def find_setup_for_support(df: pd.DataFrame,\n",
    "                           support_level: float,\n",
    "                           start_pos: int,     # start from the LATER pivot's position\n",
    "                           n: int = 20,\n",
    "                           far_pct: float = 0.01,    # 1%\n",
    "                           pivot_col: str = 'isPivot'):\n",
    "    \"\"\"\n",
    "    In the next n bars starting at start_pos:\n",
    "      1) Find PL1: a pivot low >2% below support.\n",
    "      2) Let PH1 be the pivot high before PL1.\n",
    "      3) Find PH2 after PL1 such that PH2 > PH1 (within the same n-bar window).\n",
    "      If found: return dict with PL1, PH2, buy_price at 0.618 retracement.\n",
    "    Otherwise: return None.\n",
    "    \"\"\"\n",
    "    end_pos = min(len(df) - 1, start_pos + n)\n",
    "    pl1_pos = None\n",
    "    pl1_price = None\n",
    "\n",
    "    # 1) find PL1\n",
    "    for pos in range(start_pos, end_pos + 1):\n",
    "        label = df.index[pos]\n",
    "        piv = int(df.at[label, pivot_col])\n",
    "        if piv in (2, 3):\n",
    "            low_px = float(df.at[label, 'low'])\n",
    "            if (support_level - low_px) / support_level > far_pct:\n",
    "                pl1_pos, pl1_price = pos, low_px\n",
    "                break\n",
    "\n",
    "    if pl1_pos is None:\n",
    "        return None  # no break below support\n",
    "\n",
    "    # 2) PH1: pivot high immediately before PL1\n",
    "    ph1_pos, ph1_price = prev_pivot_high_before(df, pl1_pos, pivot_col=pivot_col)\n",
    "    if ph1_pos is None or ph1_price is None:\n",
    "        return None  # cannot define PH1\n",
    "\n",
    "    # 3) find PH2 after PL1 that exceeds PH1\n",
    "    for pos in range(pl1_pos + 1, end_pos + 1):\n",
    "        label = df.index[pos]\n",
    "        piv = int(df.at[label, pivot_col])\n",
    "        if piv in (1, 3):\n",
    "            high_px = float(df.at[label, 'high'])\n",
    "            if high_px > ph1_price:\n",
    "                ph2_pos, ph2_price = pos, high_px\n",
    "                # Fibonacci buy at 0.618 retracement of (PL1 -> PH2)\n",
    "                buy_price = ph2_price - 0.618 * (ph2_price - pl1_price)\n",
    "                return {\n",
    "                    'support_level': support_level,\n",
    "                    'start_pos': start_pos,\n",
    "                    'end_pos': pos,\n",
    "                    'pl1_pos': pl1_pos,\n",
    "                    'pl1_price': pl1_price,\n",
    "                    'ph1_pos': ph1_pos,\n",
    "                    'ph1_price': ph1_price,\n",
    "                    'ph2_pos': ph2_pos,\n",
    "                    'ph2_price': ph2_price,\n",
    "                    'buy_price_0618': buy_price,\n",
    "                    'tradable_setup': 1\n",
    "                }\n",
    "\n",
    "    return None\n",
    "\n",
    "# ---------- Orchestrator ----------\n",
    "def run_liquidity_gap_fvg(df: pd.DataFrame,\n",
    "                          tol_pct_level: float = 0.0025,  # 0.5% for defining level\n",
    "                          n_window: int = 20,\n",
    "                          far_pct_break: float = 0.01,   # 1% break\n",
    "                          pivot_col: str = 'isPivot'):\n",
    "    \"\"\"\n",
    "    Implements your new logic for both Step 1 and Step 2 (support side).\n",
    "    Returns:\n",
    "      out_df: input with helper columns,\n",
    "      levels: dict with 'supports' and 'resistances',\n",
    "      setups: list of detected setups (support path)\n",
    "    \"\"\"\n",
    "    df = prep_df(df)\n",
    "    df = compute_pivots_if_missing(df, 2, 1 ,pivot_col=pivot_col)\n",
    "\n",
    "    # Step 1: find support/resistance from close pivot pairs\n",
    "    supports, resistances = find_close_pivot_levels(df, tol_pct=tol_pct_level, pivot_col=pivot_col)\n",
    "\n",
    "    # For each support, Step 2: look ahead n bars from the later pivot of the pair\n",
    "    setups = []\n",
    "    for s in supports:\n",
    "        start_pos = s['second_idx']  # later pivotâ€™s position\n",
    "        res = find_setup_for_support(\n",
    "            df,\n",
    "            support_level=s['level'],\n",
    "            start_pos=start_pos,\n",
    "            n=n_window,\n",
    "            far_pct=far_pct_break,\n",
    "            pivot_col=pivot_col\n",
    "        )\n",
    "        if res:\n",
    "            # attach some provenance from the level\n",
    "            res['support_first_pos']  = s['first_idx']\n",
    "            res['support_second_pos'] = s['second_idx']\n",
    "            res['support_first_price']  = s['first_price']\n",
    "            res['support_second_price'] = s['second_price']\n",
    "            setups.append(res)\n",
    "\n",
    "    # Optional: convenience columns for quick plotting/inspection (Debugging)\n",
    "    out = df.copy()\n",
    "    out['close_pivot_support'] = np.nan\n",
    "    out['close_pivot_resist']  = np.nan\n",
    "    for s in supports:\n",
    "        out.at[out.index[s['second_idx']], 'close_pivot_support'] = s['level']\n",
    "    for r in resistances:\n",
    "        out.at[out.index[r['second_idx']], 'close_pivot_resist'] = r['level']\n",
    "\n",
    "    levels = {'supports': supports, 'resistances': resistances}\n",
    "    return out, levels, setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e9746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p8/by67wr7139zdvwbp6501h9lh0000gn/T/ipykernel_64378/3504307515.py:10: DeprecationWarning:\n",
      "\n",
      "is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supports found: 10646, Resistances found: 11620\n",
      "Tradable setups found: 328\n",
      "{'support_level': 0.162685, 'start_pos': 204939, 'end_pos': 204958, 'pl1_pos': 204954, 'pl1_price': 0.16098, 'ph1_pos': 204949, 'ph1_price': 0.16336, 'ph2_pos': 204958, 'ph2_price': 0.16384, 'buy_price_0618': 0.16207252000000003, 'tradable_setup': 1, 'support_first_pos': 204932, 'support_second_pos': 204939, 'support_first_price': 0.16275, 'support_second_price': 0.16262}\n",
      "Buy @ 0.618: 0.16207252000000003\n"
     ]
    }
   ],
   "source": [
    "# Ensure your df (e.g., df_BTC) already has OHLC + timestamp\n",
    "df_DOGE = prep_df(df_DOGE)\n",
    "df_DOGE = compute_pivots_if_missing(df_DOGE, window=2, min_gap=1)\n",
    "\n",
    "out_df, levels, setups = run_liquidity_gap_fvg(\n",
    "    df_DOGE,\n",
    "    tol_pct_level=0.0025,   # Â±0.25% to define support/resistance via two close pivots\n",
    "    n_window=20,           # scan window after support\n",
    "    far_pct_break=0.01,    # PL1 must be >1% below support\n",
    "    pivot_col='isPivot'\n",
    ")\n",
    "\n",
    "print(f\"Supports found: {len(levels['supports'])}, Resistances found: {len(levels['resistances'])}\")\n",
    "print(f\"Tradable setups found: {len(setups)}\")\n",
    "if setups:\n",
    "    print(setups[-1])\n",
    "    # Buy price (0.618)\n",
    "    print(\"Buy @ 0.618:\", setups[-1]['buy_price_0618'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35326a26",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1a8ab880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_liquidity_fvg_window(out_df, setups, supports, row_start=None, row_end=None,\n",
    "                              title=\"DOGE â€” Liquidity Gap / FVG Strategy\"):\n",
    "    df_full = out_df.sort_values('timestamp')\n",
    "    if row_start is None: row_start = 0\n",
    "    if row_end   is None: row_end   = len(df_full)\n",
    "    dfp = df_full.iloc[row_start:row_end]   # DO NOT reset_index\n",
    "\n",
    "    # Pivot markers on the visible slice\n",
    "    piv_high = np.where(dfp['isPivot'].isin([1,3]), dfp['high'] * 1.001, np.nan)\n",
    "    piv_low  = np.where(dfp['isPivot'].isin([2,3]), dfp['low']  * 0.999, np.nan)\n",
    "\n",
    "    # Visible time bounds\n",
    "    t_min = dfp['timestamp'].min()\n",
    "    t_max = dfp['timestamp'].max()\n",
    "\n",
    "    # Helpers\n",
    "    def _pos_to_time(pos):\n",
    "        if pos is None: return None\n",
    "        pos = int(pos)\n",
    "        if 0 <= pos < len(df_full):\n",
    "            return df_full.iloc[pos]['timestamp']\n",
    "        return None\n",
    "\n",
    "    def _clip_segment(a, b, lo, hi):\n",
    "        \"\"\"Return clipped [x0,x1] within [lo,hi], or (None,None) if no overlap.\"\"\"\n",
    "        x0, x1 = min(a, b), max(a, b)\n",
    "        x0c, x1c = max(x0, lo), min(x1, hi)\n",
    "        return (x0c, x1c) if x0c <= x1c else (None, None)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Candlesticks\n",
    "    fig.add_trace(go.Candlestick(\n",
    "        x=dfp['timestamp'],\n",
    "        open=dfp['open'], high=dfp['high'],\n",
    "        low=dfp['low'], close=dfp['close'],\n",
    "        name='Candles'\n",
    "    ))\n",
    "\n",
    "    # Pivot highs / lows\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dfp['timestamp'], y=piv_high,\n",
    "        mode='markers', name='Pivot High',\n",
    "        marker=dict(symbol='triangle-up', color='red', size=8)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dfp['timestamp'], y=piv_low,\n",
    "        mode='markers', name='Pivot Low',\n",
    "        marker=dict(symbol='triangle-down', color='lime', size=8)\n",
    "    ))\n",
    "\n",
    "    # Support lines ONLY between the two pivots (clipped to visible window)\n",
    "    if supports:\n",
    "        for s in supports:\n",
    "            # prefer stored times; fall back to positions if needed\n",
    "            t1 = s.get('first_time')\n",
    "            t2 = s.get('second_time')\n",
    "            if t1 is None or t2 is None:\n",
    "                t1 = _pos_to_time(s.get('first_idx'))\n",
    "                t2 = _pos_to_time(s.get('second_idx'))\n",
    "            if t1 is None or t2 is None:\n",
    "                continue\n",
    "\n",
    "            x0, x1 = _clip_segment(t1, t2, t_min, t_max)\n",
    "            if x0 is None:  # no overlap with window\n",
    "                continue\n",
    "\n",
    "            lvl = float(s['level'])\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[x0, x1],\n",
    "                y=[lvl, lvl],\n",
    "                mode='lines',\n",
    "                line=dict(color='green', dash='dot', width=1.8),\n",
    "                name='Support',\n",
    "                hovertemplate=f\"Support {lvl:.2f}<extra></extra>\",\n",
    "                showlegend=False  # avoid legend spam for many supports\n",
    "            ))\n",
    "\n",
    "    # BUY points (0.618 Fib at PH2 time)\n",
    "    if setups:\n",
    "        buy_times, buy_prices = [], []\n",
    "        for st in setups:\n",
    "            ph2_time = st.get('ph2_time') or _pos_to_time(st.get('ph2_pos'))\n",
    "            buy_price = st.get('buy_price_0618')\n",
    "            if ph2_time is None or buy_price is None:\n",
    "                continue\n",
    "            if t_min <= ph2_time <= t_max:\n",
    "                buy_times.append(ph2_time)\n",
    "                buy_prices.append(buy_price)\n",
    "\n",
    "                # Optional: PL1â†”PH2 fib box for context\n",
    "                pl1_time = st.get('pl1_time') or _pos_to_time(st.get('pl1_pos'))\n",
    "                pl1_price = st.get('pl1_price'); ph2_price = st.get('ph2_price')\n",
    "                if pl1_time and pl1_price and ph2_price:\n",
    "                    x0, x1 = _clip_segment(pl1_time, ph2_time, t_min, t_max)\n",
    "                    if x0 is not None:\n",
    "                        fig.add_shape(\n",
    "                            type=\"rect\", xref=\"x\", yref=\"y\",\n",
    "                            x0=x0, x1=x1,\n",
    "                            y0=min(pl1_price, ph2_price), y1=max(pl1_price, ph2_price),\n",
    "                            line=dict(color=\"gold\", width=1, dash=\"dot\"),\n",
    "                            fillcolor=\"rgba(255,215,0,0.05)\"\n",
    "                        )\n",
    "\n",
    "        if buy_times:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=buy_times, y=buy_prices,\n",
    "                mode='markers+text',\n",
    "                name='Buy (0.618 Fib)',\n",
    "                text=[\"BUY\"] * len(buy_times),\n",
    "                textposition=\"bottom center\",\n",
    "                marker=dict(symbol='star', size=12, color='gold'),\n",
    "                hovertemplate=\"BUY @ 0.618 Fib<br>%{x|%Y-%m-%d %H:%M}<br>Price: %{y:.2f}<extra></extra>\"\n",
    "            ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Time\", yaxis_title=\"Price\",\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='left', x=0),\n",
    "        plot_bgcolor=\"white\"\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p8/by67wr7139zdvwbp6501h9lh0000gn/T/ipykernel_64378/1004059385.py:10: DeprecationWarning:\n",
      "\n",
      "is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "\n",
      "/var/folders/p8/by67wr7139zdvwbp6501h9lh0000gn/T/ipykernel_64378/1004059385.py:10: DeprecationWarning:\n",
      "\n",
      "is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supports found: 2556, Resistances found: 2833\n",
      "Tradable setups found: 31\n",
      "{'support_level': 68355.305, 'start_pos': 42432, 'end_pos': 42448, 'pl1_pos': 42445, 'pl1_price': 66835.0, 'ph1_pos': 42443, 'ph1_price': 68100.0, 'ph2_pos': 42448, 'ph2_price': 68180.0, 'buy_price_0618': 67348.79, 'tradable_setup': 1, 'support_first_pos': 42424, 'support_second_pos': 42432, 'support_first_price': 68240.61, 'support_second_price': 68470.0}\n",
      "Buy @ 0.618: 67348.79\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93683ac3",
   "metadata": {},
   "source": [
    "# Plotting to Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "545524bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_liquidity_fvg_window(out_df, setups, supports, row_start=None, row_end=None,\n",
    "                              title=\"BTC â€” Liquidity Gap / FVG Strategy\"):\n",
    "    df_full = out_df.sort_values('timestamp')\n",
    "    if row_start is None: row_start = 0\n",
    "    if row_end   is None: row_end   = len(df_full)\n",
    "    dfp = df_full.iloc[row_start:row_end]   # DO NOT reset_index\n",
    "\n",
    "    # Pivot markers on the visible slice\n",
    "    piv_high = np.where(dfp['isPivot'].isin([1,3]), dfp['high'] * 1.001, np.nan)\n",
    "    piv_low  = np.where(dfp['isPivot'].isin([2,3]), dfp['low']  * 0.999, np.nan)\n",
    "\n",
    "    # Visible time bounds\n",
    "    t_min = dfp['timestamp'].min()\n",
    "    t_max = dfp['timestamp'].max()\n",
    "\n",
    "    # Helpers\n",
    "    def _pos_to_time(pos):\n",
    "        if pos is None: return None\n",
    "        pos = int(pos)\n",
    "        if 0 <= pos < len(df_full):\n",
    "            return df_full.iloc[pos]['timestamp']\n",
    "        return None\n",
    "\n",
    "    def _clip_segment(a, b, lo, hi):\n",
    "        \"\"\"Return clipped [x0,x1] within [lo,hi], or (None,None) if no overlap.\"\"\"\n",
    "        x0, x1 = min(a, b), max(a, b)\n",
    "        x0c, x1c = max(x0, lo), min(x1, hi)\n",
    "        return (x0c, x1c) if x0c <= x1c else (None, None)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Candlesticks\n",
    "    fig.add_trace(go.Candlestick(\n",
    "        x=dfp['timestamp'],\n",
    "        open=dfp['open'], high=dfp['high'],\n",
    "        low=dfp['low'], close=dfp['close'],\n",
    "        name='Candles'\n",
    "    ))\n",
    "\n",
    "    # Pivot highs / lows\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dfp['timestamp'], y=piv_high,\n",
    "        mode='markers', name='Pivot High',\n",
    "        marker=dict(symbol='triangle-up', color='red', size=8)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=dfp['timestamp'], y=piv_low,\n",
    "        mode='markers', name='Pivot Low',\n",
    "        marker=dict(symbol='triangle-down', color='lime', size=8)\n",
    "    ))\n",
    "\n",
    "    # Support lines ONLY between the two pivots (clipped to visible window)\n",
    "    if supports:\n",
    "        for s in supports:\n",
    "            # prefer stored times; fall back to positions if needed\n",
    "            t1 = s.get('first_time')\n",
    "            t2 = s.get('second_time')\n",
    "            if t1 is None or t2 is None:\n",
    "                t1 = _pos_to_time(s.get('first_idx'))\n",
    "                t2 = _pos_to_time(s.get('second_idx'))\n",
    "            if t1 is None or t2 is None:\n",
    "                continue\n",
    "\n",
    "            x0, x1 = _clip_segment(t1, t2, t_min, t_max)\n",
    "            if x0 is None:  # no overlap with window\n",
    "                continue\n",
    "\n",
    "            lvl = float(s['level'])\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[x0, x1],\n",
    "                y=[lvl, lvl],\n",
    "                mode='lines',\n",
    "                line=dict(color='green', dash='dot', width=1.8),\n",
    "                name='Support',\n",
    "                hovertemplate=f\"Support {lvl:.2f}<extra></extra>\",\n",
    "                showlegend=False  # avoid legend spam for many supports\n",
    "            ))\n",
    "\n",
    "    # BUY points (0.618 Fib at PH2 time)\n",
    "    if setups:\n",
    "        buy_times, buy_prices = [], []\n",
    "        for st in setups:\n",
    "            ph2_time = st.get('ph2_time') or _pos_to_time(st.get('ph2_pos'))\n",
    "            buy_price = st.get('buy_price_0618')\n",
    "            if ph2_time is None or buy_price is None:\n",
    "                continue\n",
    "            if t_min <= ph2_time <= t_max:\n",
    "                buy_times.append(ph2_time)\n",
    "                buy_prices.append(buy_price)\n",
    "\n",
    "                # Optional: PL1â†”PH2 fib box for context\n",
    "                pl1_time = st.get('pl1_time') or _pos_to_time(st.get('pl1_pos'))\n",
    "                pl1_price = st.get('pl1_price'); ph2_price = st.get('ph2_price')\n",
    "                if pl1_time and pl1_price and ph2_price:\n",
    "                    x0, x1 = _clip_segment(pl1_time, ph2_time, t_min, t_max)\n",
    "                    if x0 is not None:\n",
    "                        fig.add_shape(\n",
    "                            type=\"rect\", xref=\"x\", yref=\"y\",\n",
    "                            x0=x0, x1=x1,\n",
    "                            y0=min(pl1_price, ph2_price), y1=max(pl1_price, ph2_price),\n",
    "                            line=dict(color=\"gold\", width=1, dash=\"dot\"),\n",
    "                            fillcolor=\"rgba(255,215,0,0.05)\"\n",
    "                        )\n",
    "\n",
    "        if buy_times:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=buy_times, y=buy_prices,\n",
    "                mode='markers+text',\n",
    "                name='Buy (0.618 Fib)',\n",
    "                text=[\"BUY\"] * len(buy_times),\n",
    "                textposition=\"bottom center\",\n",
    "                marker=dict(symbol='star', size=12, color='gold'),\n",
    "                hovertemplate=\"BUY @ 0.618 Fib<br>%{x|%Y-%m-%d %H:%M}<br>Price: %{y:.2f}<extra></extra>\"\n",
    "            ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Time\", yaxis_title=\"Price\",\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='left', x=0),\n",
    "        plot_bgcolor=\"white\"\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8171ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use FULL out_df, not the reset-index slice\n",
    "fig = plot_liquidity_fvg_window(\n",
    "    out_df,                       # full df from run_liquidity_gap_fvg\n",
    "    setups=setups,                # setups list from run_liquidity_gap_fvg\n",
    "    supports=levels['supports'],  # supports list from run_liquidity_gap_fvg\n",
    "    row_start=500, row_end=5000  # pick your window safely\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5be387",
   "metadata": {},
   "source": [
    "# BackTesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fe440e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _find_entry_after_ph2(df: pd.DataFrame, ph2_pos: int, entry_price: float):\n",
    "    \"\"\"\n",
    "    Scan forward from ph2_pos for the first bar whose range includes entry_price.\n",
    "    Returns (entry_pos, entry_time) or (None, None) if never touched.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    for pos in range(ph2_pos + 1, n):\n",
    "        low = float(df.iloc[pos]['low'])\n",
    "        high = float(df.iloc[pos]['high'])\n",
    "        if low <= entry_price <= high:\n",
    "            t = df.iloc[pos]['timestamp'] if 'timestamp' in df.columns else pos\n",
    "            return pos, t\n",
    "    return None, None\n",
    "\n",
    "def _simulate_trade_long(df: pd.DataFrame,\n",
    "                         entry_pos: int,\n",
    "                         entry_price: float,\n",
    "                         pl1_price: float,\n",
    "                         support_level: float,\n",
    "                         ph2_price: float):\n",
    "    \"\"\"\n",
    "    Simulate the management rules on bars AFTER entry_pos.\n",
    "    Returns a dict with realized return, flags (tp1/2/3 hit), and exit info.\n",
    "    \"\"\"\n",
    "    # Levels\n",
    "    tp1 = ph2_price\n",
    "    tp2 = (ph2_price - pl1_price) * 1.618 + pl1_price\n",
    "    tp3 = (ph2_price - pl1_price) * 2.618 + pl1_price\n",
    "\n",
    "    # Initial stop: average of PL1 and support\n",
    "    stop = (pl1_price + support_level) / 2.0\n",
    "\n",
    "    # Position sizing (total = 1.0 â€œall capitalâ€)\n",
    "    remaining = 1.0\n",
    "    realized = 0.0\n",
    "\n",
    "    # Flags\n",
    "    hit_tp1 = False\n",
    "    hit_tp2 = False\n",
    "    hit_tp3 = False\n",
    "\n",
    "    # After TP1, stop moves to ENTRY; after TP2, stop moves to PH2\n",
    "    n = len(df)\n",
    "    for pos in range(entry_pos + 1, n):\n",
    "        low = float(df.iloc[pos]['low'])\n",
    "        high = float(df.iloc[pos]['high'])\n",
    "        time = df.iloc[pos]['timestamp'] if 'timestamp' in df.columns else pos\n",
    "\n",
    "        # SEQUENCE: for a long, evaluate downside (stop) before upside (TP) per bar\n",
    "        # (conservative fill assumption)\n",
    "        if not hit_tp1:\n",
    "            # Stop check\n",
    "            if low <= stop:\n",
    "                # exit ALL at stop\n",
    "                realized += remaining * ((stop / entry_price) - 1.0)\n",
    "                remaining = 0.0\n",
    "                return {\n",
    "                    'exit_pos': pos, 'exit_time': time, 'exit_price': stop,\n",
    "                    'ret': realized,\n",
    "                    'hit_tp1': hit_tp1, 'hit_tp2': hit_tp2, 'hit_tp3': hit_tp3\n",
    "                }\n",
    "            # TP1 check\n",
    "            if high >= tp1:\n",
    "                # sell 50%\n",
    "                sell_frac = 0.5 * remaining\n",
    "                realized += sell_frac * ((tp1 / entry_price) - 1.0)\n",
    "                remaining -= sell_frac\n",
    "                hit_tp1 = True\n",
    "                # move stop to entry\n",
    "                stop = entry_price\n",
    "                # continue to next bar\n",
    "                continue\n",
    "\n",
    "        elif hit_tp1 and not hit_tp2:\n",
    "            # Stop @ entry\n",
    "            if low <= stop:\n",
    "                # exit remaining at entry (breakeven)\n",
    "                realized += remaining * ((stop / entry_price) - 1.0)\n",
    "                remaining = 0.0\n",
    "                return {\n",
    "                    'exit_pos': pos, 'exit_time': time, 'exit_price': stop,\n",
    "                    'ret': realized,\n",
    "                    'hit_tp1': hit_tp1, 'hit_tp2': hit_tp2, 'hit_tp3': hit_tp3\n",
    "                }\n",
    "            # TP2\n",
    "            if high >= tp2:\n",
    "                sell_frac = 0.5 * remaining  # half of the remainder\n",
    "                realized += sell_frac * ((tp2 / entry_price) - 1.0)\n",
    "                remaining -= sell_frac\n",
    "                hit_tp2 = True\n",
    "                # move stop to PH2\n",
    "                stop = ph2_price\n",
    "                continue\n",
    "\n",
    "        elif hit_tp2 and not hit_tp3:\n",
    "            # Stop @ PH2\n",
    "            if low <= stop:\n",
    "                # exit remaining at PH2\n",
    "                realized += remaining * ((stop / entry_price) - 1.0)\n",
    "                remaining = 0.0\n",
    "                return {\n",
    "                    'exit_pos': pos, 'exit_time': time, 'exit_price': stop,\n",
    "                    'ret': realized,\n",
    "                    'hit_tp1': hit_tp1, 'hit_tp2': hit_tp2, 'hit_tp3': hit_tp3\n",
    "                }\n",
    "            # TP3\n",
    "            if high >= tp3:\n",
    "                # exit ALL remaining\n",
    "                realized += remaining * ((tp3 / entry_price) - 1.0)\n",
    "                remaining = 0.0\n",
    "                hit_tp3 = True\n",
    "                return {\n",
    "                    'exit_pos': pos, 'exit_time': time, 'exit_price': tp3,\n",
    "                    'ret': realized,\n",
    "                    'hit_tp1': hit_tp1, 'hit_tp2': hit_tp2, 'hit_tp3': hit_tp3\n",
    "                }\n",
    "\n",
    "    # If we reach here: no exit rule hit before data ended â€” close at last close\n",
    "    last_close = float(df.iloc[-1]['close'])\n",
    "    realized += remaining * ((last_close / entry_price) - 1.0)\n",
    "    return {\n",
    "        'exit_pos': len(df) - 1,\n",
    "        'exit_time': df.iloc[-1]['timestamp'] if 'timestamp' in df.columns else len(df) - 1,\n",
    "        'exit_price': last_close,\n",
    "        'ret': realized,\n",
    "        'hit_tp1': hit_tp1, 'hit_tp2': hit_tp2, 'hit_tp3': hit_tp3\n",
    "    }\n",
    "\n",
    "def backtest_liquidity_fvg(out_df: pd.DataFrame, setups: list):\n",
    "    \"\"\"\n",
    "    Run the trade simulation across all setups (in chronological order).\n",
    "    Returns (trades_df, summary_stats).\n",
    "    \"\"\"\n",
    "    if not setups:\n",
    "        return pd.DataFrame(), {\n",
    "            'final_return': 0.0, 'average_return': 0.0, 'success_rate': 0.0,\n",
    "            'tp1_hits': 0, 'tp2_hits': 0, 'tp3_hits': 0, 'max_drawdown': 0.0,\n",
    "            'n_trades': 0\n",
    "        }\n",
    "\n",
    "    # Ensure chronological order by PH2 (confirmation) time/pos\n",
    "    df = out_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    enriched = []\n",
    "    for s in setups:\n",
    "        # Positions are positional (from earlier code). Safety clamps:\n",
    "        ph2_pos = int(s['ph2_pos'])\n",
    "        pl1_pos = int(s['pl1_pos'])\n",
    "        if ph2_pos >= len(df) or pl1_pos >= len(df):  # skip out-of-range\n",
    "            continue\n",
    "\n",
    "        entry_price = s['buy_price_0618']\n",
    "        entry_pos, entry_time = _find_entry_after_ph2(df, ph2_pos, entry_price)\n",
    "        if entry_pos is None:\n",
    "            enriched.append({\n",
    "                'status': 'no_entry',\n",
    "                'entry_time': None, 'entry_pos': None, 'entry_price': entry_price,\n",
    "                'pl1_price': s['pl1_price'],\n",
    "                'support_level': s['support_level'],\n",
    "                'ph2_price': s['ph2_price'],\n",
    "                'ret': 0.0,\n",
    "                'hit_tp1': False, 'hit_tp2': False, 'hit_tp3': False\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # simulate\n",
    "        result = _simulate_trade_long(\n",
    "            df=df,\n",
    "            entry_pos=entry_pos,\n",
    "            entry_price=entry_price,\n",
    "            pl1_price=s['pl1_price'],\n",
    "            support_level=s['support_level'],\n",
    "            ph2_price=s['ph2_price']\n",
    "        )\n",
    "\n",
    "        enriched.append({\n",
    "            'status': 'filled',\n",
    "            'entry_time': entry_time, 'entry_pos': entry_pos, 'entry_price': entry_price,\n",
    "            'pl1_price': s['pl1_price'],\n",
    "            'support_level': s['support_level'],\n",
    "            'ph2_price': s['ph2_price'],\n",
    "            'exit_time': result['exit_time'], 'exit_pos': result['exit_pos'], 'exit_price': result['exit_price'],\n",
    "            'ret': result['ret'],\n",
    "            'hit_tp1': result['hit_tp1'], 'hit_tp2': result['hit_tp2'], 'hit_tp3': result['hit_tp3']\n",
    "        })\n",
    "\n",
    "    trades_df = pd.DataFrame(enriched)\n",
    "\n",
    "    # Keep only executed trades for stats (status == 'filled')\n",
    "    exec_df = trades_df[trades_df['status'] == 'filled'].copy()\n",
    "    n_trades = len(exec_df)\n",
    "\n",
    "    if n_trades == 0:\n",
    "        return trades_df, {\n",
    "            'final_return': 0.0, 'average_return': 0.0, 'success_rate': 0.0,\n",
    "            'tp1_hits': 0, 'tp2_hits': 0, 'tp3_hits': 0, 'max_drawdown': 0.0,\n",
    "            'n_trades': 0\n",
    "        }\n",
    "\n",
    "    # Returns are per-trade percentage returns (e.g., 0.12 = +12%)\n",
    "    avg_ret = float(exec_df['ret'].mean())\n",
    "    success_rate = float((exec_df['ret'] > 0).mean())\n",
    "\n",
    "    # Count how many trades hit each TP at any time in the lifecycle\n",
    "    tp1_hits = int(exec_df['hit_tp1'].sum())\n",
    "    tp2_hits = int(exec_df['hit_tp2'].sum())\n",
    "    tp3_hits = int(exec_df['hit_tp3'].sum())\n",
    "\n",
    "    # Equity curve (compounded, sequential) for max drawdown\n",
    "    # Assume we reinvest all equity each trade in chronological order of entry_time.\n",
    "    exec_df = exec_df.sort_values('entry_time')\n",
    "    equity = [1.0]\n",
    "    for r in exec_df['ret']:\n",
    "        equity.append(equity[-1] * (1.0 + r))\n",
    "    equity = np.array(equity[1:])  # drop initial 1.0 placeholder\n",
    "\n",
    "    if len(equity) == 0:\n",
    "        mdd = 0.0\n",
    "        final_ret = 0.0\n",
    "    else:\n",
    "        peaks = np.maximum.accumulate(equity)\n",
    "        drawdowns = (equity - peaks) / peaks\n",
    "        mdd = float(drawdowns.min()) if len(drawdowns) else 0.0\n",
    "        final_ret = float(equity[-1] - 1.0)\n",
    "\n",
    "    summary = {\n",
    "        'final_return': final_ret,        # compounded return across trades\n",
    "        'average_return': avg_ret,        # mean per-trade return\n",
    "        'success_rate': success_rate,     # fraction of profitable trades\n",
    "        'tp1_hits': tp1_hits,\n",
    "        'tp2_hits': tp2_hits,\n",
    "        'tp3_hits': tp3_hits,\n",
    "        'max_drawdown': mdd,              # negative value (e.g., -0.23 = -23%)\n",
    "        'n_trades': n_trades\n",
    "    }\n",
    "\n",
    "    return trades_df, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "81ff3e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Summary =====\n",
      "    final_return: 0.1802\n",
      "  average_return: 0.0007\n",
      "    success_rate: 0.2994\n",
      "        tp1_hits: 58\n",
      "        tp2_hits: 34\n",
      "        tp3_hits: 13\n",
      "    max_drawdown: -0.3024\n",
      "        n_trades: 324\n",
      "\n",
      "===== Sample trades =====\n",
      "   status          entry_time  entry_pos  entry_price  pl1_price  \\\n",
      "0  filled 2020-01-02 21:30:00      182.0     0.002001   0.001988   \n",
      "1  filled 2020-01-20 13:15:00     1877.0     0.002356   0.002327   \n",
      "2  filled 2020-01-22 02:30:00     2026.0     0.002342   0.002321   \n",
      "3  filled 2020-01-28 21:30:00     2678.0     0.002375   0.002350   \n",
      "4  filled 2020-01-28 21:30:00     2678.0     0.002375   0.002350   \n",
      "5  filled 2020-01-31 14:00:00     2936.0     0.002374   0.002354   \n",
      "6  filled 2020-02-03 02:00:00     3176.0     0.002455   0.002427   \n",
      "7  filled 2020-02-03 02:00:00     3176.0     0.002455   0.002427   \n",
      "8  filled 2020-02-10 19:15:00     3913.0     0.002925   0.002842   \n",
      "9  filled 2020-02-13 00:30:00     4126.0     0.003065   0.003045   \n",
      "\n",
      "   support_level  ph2_price           exit_time  exit_pos  exit_price  \\\n",
      "0       0.002010   0.002022 2020-01-02 22:00:00     184.0    0.001999   \n",
      "1       0.002353   0.002403 2020-01-21 19:00:00    1996.0    0.002340   \n",
      "2       0.002361   0.002376 2020-01-22 15:30:00    2078.0    0.002342   \n",
      "3       0.002380   0.002415 2020-01-28 22:30:00    2682.0    0.002365   \n",
      "4       0.002380   0.002415 2020-01-28 22:30:00    2682.0    0.002365   \n",
      "5       0.002380   0.002405 2020-01-31 14:30:00    2938.0    0.002367   \n",
      "6       0.002457   0.002500 2020-02-03 03:30:00    3182.0    0.002442   \n",
      "7       0.002453   0.002500 2020-02-03 03:30:00    3182.0    0.002440   \n",
      "8       0.002954   0.003060 2020-02-11 01:00:00    3936.0    0.002898   \n",
      "9       0.003077   0.003098 2020-02-13 00:45:00    4127.0    0.003061   \n",
      "\n",
      "        ret  hit_tp1  hit_tp2  hit_tp3  \n",
      "0 -0.000963    False    False    False  \n",
      "1 -0.006784    False    False    False  \n",
      "2  0.007310     True    False    False  \n",
      "3 -0.004139    False    False    False  \n",
      "4 -0.004118    False    False    False  \n",
      "5 -0.002693    False    False    False  \n",
      "6 -0.005177    False    False    False  \n",
      "7 -0.006114    False    False    False  \n",
      "8 -0.009334    False    False    False  \n",
      "9 -0.001487    False    False    False  \n"
     ]
    }
   ],
   "source": [
    "trades_df, summary = backtest_liquidity_fvg(out_df, setups)\n",
    "\n",
    "print(\"===== Summary =====\")\n",
    "for k, v in summary.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k:>16}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"{k:>16}: {v}\")\n",
    "\n",
    "print(\"\\n===== Sample trades =====\")\n",
    "print(trades_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544aea21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1a81464",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e32ce056",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b46fa909",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f25d531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tradable setups detected: 0\n"
     ]
    }
   ],
   "source": [
    "# After running your strategy:\n",
    "print(f\"Total tradable setups detected: {len(setups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a2d9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
